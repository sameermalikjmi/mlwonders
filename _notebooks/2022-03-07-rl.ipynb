{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Introduction\n",
    "> RL Introduction based on MDP\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [RL]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](my_icons/rl_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "np.set_printoptions(precision=3)\n",
    "# Evaluate deterministic\n",
    "register(\n",
    "    id='Deterministic-4x4-FrozenLake-v0',\n",
    "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4',\n",
    "            'is_slippery': False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-8):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    P, nS, nA, gamma:\n",
    "        defined at beginning of file\n",
    "    policy: np.array[nS,nA]\n",
    "        The policy to evaluate. Maps states to actions.\n",
    "    tol: float\n",
    "        Terminate policy evaluation when\n",
    "            max |value_function(s) - prev_value_function(s)| < tol\n",
    "    Returns:\n",
    "    -------\n",
    "    value_function: np.ndarray[nS]\n",
    "        The value function of the given policy, where value_function[s] is\n",
    "        the value of state s\n",
    "    \"\"\"\n",
    "    \n",
    "    value_function= np.zeros(nS)\n",
    "    \n",
    "  \n",
    "    while True:\n",
    "      \n",
    "        difference=0\n",
    "      \n",
    "        for s in range(nS):\n",
    "            sum = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                \n",
    "                for probability, next_state, reward, terminal in P[s][a]:\n",
    "                  \n",
    "                    sum += action_prob * probability * (reward + gamma * value_function[next_state])\n",
    "\n",
    "           \n",
    "            difference = max(difference, np.abs(value_function[s]-sum))\n",
    "\n",
    "            \n",
    "            value_function[s] = sum\n",
    "\n",
    "       \n",
    "        if difference < tol:\n",
    "            break\n",
    "\n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.004, 0.004, 0.01 , 0.004, 0.007, 0.   , 0.026, 0.   , 0.019,\n",
       "       0.058, 0.107, 0.   , 0.   , 0.13 , 0.391, 0.   ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env = env.unwrapped\n",
    "random_policy2 = np.ones([env.nS, env.nA]) / env.nA\n",
    "policy_evaluation(env.P,env.nS,env.nA, random_policy2,tol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
